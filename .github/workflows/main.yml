# Let's scrape DWD data!
name: dwd_wbi

on:
  schedule:
    - cron:  '* * * * *' # Every day at 10 am. Ref https://crontab.guru/examples.html

jobs: 
  autoscrape:
    runs-on: ubuntu-latest

    # Load repo and install R
    steps:
    - uses: actions/checkout@master
    - uses: r-lib/actions/setup-r@master
    
    # Set-up 
    - name: Setup
      run: |
        sudo apt-get install libgdal-dev libproj-dev libgeos-dev libudunits2-dev unzip
        R -e 'install.packages("devtools")'
        R -e 'devtools::install_github("ropensci/rnaturalearthhires")'
        R -e 'install.packages("tidyverse")'
        R -e 'install.packages("jsonlite")'
        R -e 'install.packages("rvest")'
        R -e 'install.packages("dplyr")'
        R -e 'install.packages("tidyr")'
        R -e 'install.packages("stringr")'
        R -e 'install.packages("glue")'
        R -e 'install.packages("purrr")'
        R -e 'install.packages("lubridate")'
        R -e 'install.packages("sf")'
        R -e 'install.packages("tidygeocoder")'
        R -e 'install.packages("readr")'
        R -e 'install.packages("rnaturalearth")'
        R -e 'install.packages("stars")'
        R -e 'install.packages("gstat")'
        R -e 'install.packages("automap")'
        R -e 'install.packages("ggplot2")'
        R -e 'install.packages("rcartocolor")'
    # Run R script
    - name: Scrape
      run: Rscript scrape_wbi.R
      env:
        GOOGLEGEOCODE_API_KEY: ${{ secrets.GOOGLEGEOCODE_API_KEY }}
      
 # Add new files in data folder, commit along with other modified files, push
    - name: Commit files
      run: |
        git config --local user.name actions-user
        git config --local user.email "actions@github.com"
        git add data/*
        git commit -am "GH ACTION Headlines $(date)"
        git push origin main
      env:
        REPO_KEY: ${{secrets.GITHUB_TOKEN}}
        username: github-actions
